{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede27b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import evaluate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    MaskFormerForInstanceSegmentation,\n",
    "    MaskFormerImageProcessor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa3240",
   "metadata": {},
   "source": [
    "# Load Model and PreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e01a33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Grab the trained model and processor from the hub\n",
    "model = MaskFormerForInstanceSegmentation.from_pretrained(\n",
    "    \"tomascanivari/maskformer-swin-base-building-instance\"\n",
    ").to(device)\n",
    "\n",
    "processor = MaskFormerImageProcessor.from_pretrained(\n",
    "    \"tomascanivari/maskformer-swin-base-building-instance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d87b18",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a400fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Converted Dataset\n",
    "DATASET_HF_DIR = \"tomascanivari/building_extraction\"\n",
    "\n",
    "# Load the whole dataset dict\n",
    "dataset = load_dataset(\"tomascanivari/building_extraction\")\n",
    "\n",
    "# Test Split Annotation is Place-Holder\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d55958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check first train image and annotation\n",
    "example = dataset[\"train\"][0]\n",
    "img = example[\"image\"]\n",
    "ann = example[\"annotation\"]\n",
    "\n",
    "# Load PIL image\n",
    "image = np.array(img.convert(\"RGB\"))\n",
    "annotation = np.array(ann)\n",
    "\n",
    "print(\"Number of Categories: \", np.unique(annotation[..., 0]))  # Red channel: category IDs\n",
    "print(\"Number of Instances: \", np.unique(annotation[..., 1]))  # Green channel: instance IDs\n",
    "\n",
    "# Plot the original image and the annotations\n",
    "plt.figure(figsize=(15, 5))\n",
    "for plot_index in range(3):\n",
    "    if plot_index == 0:\n",
    "        # If plot index is 0 display the original image\n",
    "        plot_image = image\n",
    "        title = \"Original\"\n",
    "    else:\n",
    "        # Else plot the annotation maps\n",
    "        plot_image = annotation[..., plot_index - 1]\n",
    "        title = [\"Class Map (R)\", \"Instance Map (G)\"][plot_index - 1]\n",
    "    # Plot the image\n",
    "    plt.subplot(1, 3, plot_index + 1)\n",
    "    plt.imshow(plot_image)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# Let' check instance 0\n",
    "print(\"Instance 1\")\n",
    "mask = (annotation[..., 1] == 1)\n",
    "visual_mask = (mask * 255).astype(np.uint8)\n",
    "Image.fromarray(visual_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f316e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RLE obtained from instance segmentation annotation\n",
    "from pycocotools import mask as mask_utils\n",
    "\n",
    "def instance_mask_to_rle(instance_mask):\n",
    "    \"\"\"\n",
    "    annotation_image: H x W x 3\n",
    "        - red channel: instance ID\n",
    "        - green channel: class label\n",
    "    Returns:\n",
    "        List of RLEs (one per instance)\n",
    "    \"\"\"\n",
    "    instance_ids = np.unique(instance_mask)\n",
    "    instance_ids = instance_ids[instance_ids != 255]  # exclude background\n",
    "    \n",
    "    rles = []\n",
    "    for inst_id in instance_ids:\n",
    "        mask = (instance_mask == inst_id).astype(np.uint8)\n",
    "        rle = mask_utils.encode(np.asfortranarray(mask))\n",
    "        rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")  # optional for JSON compatibility\n",
    "        rles.append(rle)\n",
    "    return rles\n",
    "\n",
    "def visualize_rle_on_image(image, rle_list, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Visualize RLE masks over the original image.\n",
    "    \n",
    "    Args:\n",
    "        image: H x W x 3 NumPy array (original image)\n",
    "        rle_list: list of RLEs (from pycocotools)\n",
    "        alpha: transparency for mask overlay\n",
    "    \"\"\"\n",
    "    overlay = image.copy()\n",
    "    \n",
    "    for rle in rle_list:\n",
    "        mask = mask_utils.decode(rle)  # H x W, 0/1\n",
    "        color = np.array([0, 0, 255], dtype=np.uint8)\n",
    "        overlay[mask==1] = (1-alpha)*overlay[mask==1] + alpha*color\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "idx = 1\n",
    "\n",
    "image = np.array(dataset[\"val\"][idx][\"image\"].convert(\"RGB\"))\n",
    "\n",
    "annotation = np.array(dataset[\"val\"][idx][\"annotation\"])\n",
    "annotation -= 1  # Reduce labels\n",
    "annotation[annotation == -1] = 255  # ignore_index\n",
    "\n",
    "rles = instance_mask_to_rle(annotation[..., 1])\n",
    "\n",
    "print(len(rles), len(np.unique(annotation[..., 1]))-1)\n",
    "\n",
    "visualize_rle_on_image(image, rles)\n",
    "\n",
    "# Plot the original image and the annotations\n",
    "plt.figure(figsize=(15, 5))\n",
    "for plot_index in range(3):\n",
    "    if plot_index == 0:\n",
    "        # If plot index is 0 display the original image\n",
    "        plot_image = image\n",
    "        title = \"Original\"\n",
    "    else:\n",
    "        # Else plot the annotation maps\n",
    "        plot_image = annotation[..., plot_index - 1]\n",
    "        title = [\"Class Map (R)\", \"Instance Map (G)\"][plot_index - 1]\n",
    "    # Plot the image\n",
    "    plt.subplot(1, 3, plot_index + 1)\n",
    "    plt.imshow(plot_image)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e056fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the prediction on the first test image\n",
    "\n",
    "image = dataset[\"val\"][idx][\"image\"].convert(\"RGB\")\n",
    "target_size = image.size[::-1]\n",
    "\n",
    "# Preprocess image\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Let's print the items returned by our model and their shapes\n",
    "print(\"Outputs...\")\n",
    "for key, value in outputs.items():\n",
    "    print(f\"  {key}: {value.shape}\")\n",
    "\n",
    "# Post-process results to retrieve instance segmentation maps\n",
    "result = processor.post_process_instance_segmentation(\n",
    "    outputs,\n",
    "    threshold=0.5,\n",
    "    target_sizes=[target_size], \n",
    ")[0] # we pass a single output therefore we take the first result (single)\n",
    "\n",
    "instance_seg_mask = result[\"segmentation\"].cpu().detach().numpy()\n",
    "instance_seg_mask[instance_seg_mask == -1] = 255\n",
    "\n",
    "# for i in range(instance_seg_mask.shape[0]):\n",
    "#     for j in range(instance_seg_mask.shape[1]):\n",
    "#         print(instance_seg_mask[i][j], end=' ')\n",
    "#     print()\n",
    "\n",
    "rles = instance_mask_to_rle(instance_seg_mask)\n",
    "\n",
    "print(len(rles), len(np.unique(instance_seg_mask)-1))\n",
    "\n",
    "visualize_rle_on_image(np.array(image), rles)\n",
    "\n",
    "\n",
    "print(f\"Final mask shape: {instance_seg_mask.shape}\")\n",
    "print(\"Segments Information...\")\n",
    "for info in result[\"segments_info\"]:\n",
    "    print(f\"  {info}\")\n",
    "\n",
    "print(np.unique(instance_seg_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_rles = {}\n",
    "pred_rles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mean IoU metric\n",
    "metrics = evaluate.load(\"mean_iou\")\n",
    "\n",
    "# Set model in evaluation mode\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Test set doesn't have annotations, so we use validation\n",
    "ground_truths, preds = [], []\n",
    "\n",
    "for idx in tqdm(range(len(dataset[\"val\"]))):\n",
    "    data = dataset[\"val\"][idx]\n",
    "    image = data[\"image\"].convert(\"RGB\")\n",
    "    target_size = image.size[::-1]\n",
    "\n",
    "    # Ground truth semantic segmentation map\n",
    "    annotation = np.array(data[\"annotation\"])[:, :, 1]  # make sure indexing is per-sample\n",
    "    annotation -= 1  # Reduce labels\n",
    "    annotation[annotation == -1] = 255  # ignore_index\n",
    "    ground_truths.append(annotation)\n",
    "\n",
    "    # Preprocess image\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Post-process semantic segmentation\n",
    "    result = processor.post_process_semantic_segmentation(\n",
    "        outputs, target_sizes=[target_size]\n",
    "    )[0]\n",
    "    semantic_seg_mask = result.cpu().numpy()\n",
    "    preds.append(semantic_seg_mask)\n",
    "\n",
    "# Compute metric\n",
    "results = metrics.compute(\n",
    "    predictions=preds,\n",
    "    references=ground_truths,\n",
    "    num_labels=2,\n",
    "    ignore_index=255\n",
    ")\n",
    "\n",
    "print(f\"Mean IoU: {results['mean_iou']} | Mean Accuracy: {results['mean_accuracy']} | Overall Accuracy: {results['overall_accuracy']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BuildingEnv",
   "language": "python",
   "name": "building_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bac73592",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f449bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install the necessary dependencies\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128 \n",
    "# !pip install datasets\n",
    "# !pip install evaluate \n",
    "# !pip install albumentations\n",
    "# !pip install git+https://github.com/huggingface/transformers.git\n",
    "\n",
    "# # # We will use this to push our trained model to HF Hub\n",
    "# !pip install huggingface_hub \n",
    "# !pip install torchmetrics \n",
    "# !pip install 'accelerate>=1.1.0'\n",
    "# !pip install matplotlib\n",
    "# !pip install pycocotools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026701d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomas/Downloads/Siemens/building_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary packages\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import DatasetDict, Dataset, load_from_disk\n",
    "from transformers import (\n",
    "    Mask2FormerConfig,\n",
    "    Mask2FormerImageProcessor,\n",
    "    Mask2FormerModel,\n",
    "    Mask2FormerForUniversalSegmentation,\n",
    "    Trainer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.trainer import EvalPrediction\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import evaluate\n",
    "from huggingface_hub import notebook_login\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Optional\n",
    "import logging\n",
    "import transformers\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45fac66",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b27a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO-style annotations from the 'buildings' dataset and convert to the Instance Segmentations format\n",
    "\n",
    "def coco2seg(dataset_dir, splits=['train', 'val', 'test']):\n",
    "    \"\"\"\n",
    "    Convert a COCO-style JSON (images, annotations, categories) to\n",
    "    a Instance Segmentation Dataset compatible format for Hugging Face Tasks.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: path to the dataset\n",
    "        splits: splits to load and convert\n",
    "    \n",
    "    Returns:\n",
    "        DatasetDict object (!!!images saved as paths for memory usage!!!):\n",
    "            - 'image': PIL.Image\n",
    "            - 'annotation': PIL.Image with\n",
    "                            R channel = category_id\n",
    "                            G channel = instance_id (unique per image, <256 instances)\n",
    "    \"\"\"\n",
    "    dataset_dir = Path(dataset_dir)\n",
    "    result = {}\n",
    "\n",
    "    for split in splits:\n",
    "        ann_path = dataset_dir / f\"{split}/{split}_512.json\"\n",
    "        img_dir = dataset_dir / split / \"image_512\"\n",
    "\n",
    "        if not ann_path.exists() or not img_dir.exists():\n",
    "            if split == \"test\" and img_dir.exists():\n",
    "                print(f\"⚠️ No annotation file found for '{split}' — loading images only.\")\n",
    "                images = []\n",
    "                for f in img_dir.glob(\"*.tif\"):\n",
    "                    images.append({\"image\": str(f)})\n",
    "                \n",
    "                result[split] = Dataset.from_list(images)\n",
    "                continue\n",
    "            print(f\"WARNING: Missing split '{split}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Processing split '{split}'...\")\n",
    "\n",
    "        # Load COCO annotation JSON\n",
    "        with open(ann_path, \"r\") as f:\n",
    "            coco = json.load(f)\n",
    "\n",
    "        images = {img[\"id\"]: img for img in coco[\"images\"]}\n",
    "        annotations = coco[\"annotations\"]\n",
    "\n",
    "        # Group annotations by image_id\n",
    "        anns_by_img = {}\n",
    "        for ann in annotations:\n",
    "            anns_by_img.setdefault(ann[\"image_id\"], []).append(ann)\n",
    "\n",
    "        records = []\n",
    "        for img_id, img_info in tqdm(images.items()):\n",
    "            file_name = Path(img_info[\"file_name\"]).name\n",
    "            \n",
    "            width, height = img_info[\"width\"], img_info[\"height\"]\n",
    "\n",
    "            image_path = img_dir / file_name\n",
    "\n",
    "            if not image_path.exists():\n",
    "                continue\n",
    "\n",
    "            # Create blank annotation image (2-channel RGB)\n",
    "            ann_img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "            # Draw polygons per instance\n",
    "            r = Image.new(\"L\", (width, height), 0)  # Category\n",
    "            g = Image.new(\"L\", (width, height), 0)  # Instance\n",
    "            draw_r = ImageDraw.Draw(r)\n",
    "            draw_g = ImageDraw.Draw(g)\n",
    "\n",
    "            anns = anns_by_img.get(img_id, [])\n",
    "            instance_counter = 1\n",
    "            cat_ids = []\n",
    "            for ann in anns:\n",
    "                \n",
    "                cat_id = int(ann[\"category_id\"])\n",
    "                cat_ids.append(cat_id)\n",
    "                polygons = ann.get(\"segmentation\", [])\n",
    "                if not polygons or not isinstance(polygons, list):\n",
    "                    continue\n",
    "\n",
    "                # Each polygon in COCO is a list of [x1, y1, x2, y2, ...]\n",
    "                for poly in polygons:\n",
    "                    if len(poly) < 6:  # invalid polygon\n",
    "                        continue\n",
    "                    xy = [(poly[i], poly[i + 1]) for i in range(0, len(poly), 2)]\n",
    "                    draw_r.polygon(xy, fill=cat_id+1)\n",
    "                    draw_g.polygon(xy, fill=instance_counter)\n",
    "\n",
    "                instance_counter += 1\n",
    "                if instance_counter >= 256:\n",
    "                    print(f\"WARNING: Too many instances in {file_name}, clipping to 255.\")\n",
    "                    break\n",
    "\n",
    "            # Merge R and G channels back into RGB\n",
    "            ann_img = np.stack([\n",
    "                np.array(r), \n",
    "                np.array(g), \n",
    "                np.zeros((height, width), np.uint8)], \n",
    "                axis=-1)\n",
    "\n",
    "            # SAVE AS PNG SUPER IMPORTANT FOR NO DATA LOSS\n",
    "            ann_path = dataset_dir / split / \"annotation\" / f\"{Path(img_info['file_name']).stem}.png\"\n",
    "            ann_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            ann_img = ann_img.astype(np.uint8)\n",
    "            Image.fromarray(ann_img).save(ann_path)\n",
    "\n",
    "            records.append({\n",
    "                \"image\": str(image_path),\n",
    "                \"annotation\": str(ann_path)\n",
    "            })\n",
    "\n",
    "        result[split] = Dataset.from_list(records)\n",
    "\n",
    "    dataset = DatasetDict(result)\n",
    "\n",
    "    dataset.save_to_disk(dataset_dir / \"hf\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "DATASET_DIR = Path(\"./building-extraction-generalization-2024\")\n",
    "\n",
    "dataset = coco2seg(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8da9196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1000 examples [00:00, 23452.30 examples/s]\n",
      "Map: 100%|██████████| 3784/3784 [00:01<00:00, 2212.73 examples/s]ards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 10.56ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████|  288MB /  288MB,  237MB/s  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-10-20T15:41:38.032081Z\u001b[0m \u001b[33m WARN\u001b[0m  \u001b[33mStatus Code: 500. Retrying..., \u001b[1;33mrequest_id\u001b[0m\u001b[33m: \"\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs:227\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (1 / 1): 100%|██████████|  288MB /  288MB, 33.0MB/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:14<00:00, 14.08s/ shards]\n",
      "Map: 100%|██████████| 933/933 [00:00<00:00, 2094.86 examples/s]shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  9.77ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 71.4MB / 71.4MB, 52.8MB/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.73s/ shards]\n",
      "Map: 100%|██████████| 250/250 [00:00<00:00, 283.20 examples/s] shards/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00,  5.14ba/s]\n",
      "Processing Files (0 / 1):  90%|█████████ |  348MB /  387MB, 6.56MB/s  \n",
      "New Data Upload: 100%|██████████|  117MB /  117MB, 4.01MB/s  \n",
      "Map: 100%|██████████| 250/250 [00:00<00:00, 449.36 examples/s]3, 27.96s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00,  5.92ba/s]\n",
      "Processing Files (0 / 1):  89%|████████▉ |  348MB /  391MB, 22.1MB/s  \n",
      "New Data Upload: 100%|██████████|  121MB /  121MB, 9.06MB/s  \n",
      "Map: 100%|██████████| 250/250 [00:00<00:00, 553.89 examples/s]7, 23.63s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00,  5.04ba/s]\n",
      "Processing Files (0 / 1):  90%|█████████ |  346MB /  384MB, 14.4MB/s  \n",
      "New Data Upload: 100%|██████████|  120MB /  120MB, 10.7MB/s  \n",
      "Map: 100%|██████████| 250/250 [00:00<00:00, 460.98 examples/s]1, 21.69s/ shards]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:00<00:00,  4.31ba/s]\n",
      "Processing Files (0 / 1):  87%|████████▋ |  324MB /  374MB, 35.2MB/s  \n",
      "New Data Upload: 100%|██████████|  110MB /  110MB, 13.1MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 4/4 [01:23<00:00, 20.98s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/tomascanivari/building_extraction/commit/3cda4b2efb754cb0d17ce56e1f500fa99aa971b1', commit_message='Upload dataset', commit_description='', oid='3cda4b2efb754cb0d17ce56e1f500fa99aa971b1', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/tomascanivari/building_extraction', endpoint='https://huggingface.co', repo_type='dataset', repo_id='tomascanivari/building_extraction'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload Dataset to Hugging Face HUB\n",
    "from datasets import Dataset, Features, Image as ImageHF\n",
    "from huggingface_hub import login\n",
    "\n",
    "login('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') \n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "# --- Define generator factory functions ---\n",
    "def make_gen_examples_train_val(images, annotations):\n",
    "    def gen():\n",
    "        for img_path, ann_path in zip(images, annotations):\n",
    "            yield {\n",
    "                \"image\": {\"path\": img_path},\n",
    "                \"annotation\": {\"path\": ann_path},\n",
    "            }\n",
    "    return gen\n",
    "\n",
    "# --- Define consistent feature schemas ---\n",
    "features = Features({\n",
    "    \"image\": ImageHF(),\n",
    "    \"annotation\": ImageHF()\n",
    "})\n",
    "\n",
    "\n",
    "# --- Build each split independently ---\n",
    "# Train\n",
    "train_images = sorted(glob.glob(\"./building-extraction-generalization-2024/train/image_512/*.jpg\"))\n",
    "train_anns = sorted(glob.glob(\"./building-extraction-generalization-2024/train/annotation/*.png\"))\n",
    "train_ds = Dataset.from_generator(make_gen_examples_train_val(train_images, train_anns),\n",
    "                                  features=features)\n",
    "\n",
    "# Validation\n",
    "val_images = sorted(glob.glob(\"./building-extraction-generalization-2024/val/image_512/*.jpg\"))\n",
    "val_anns = sorted(glob.glob(\"./building-extraction-generalization-2024/val/annotation/*.png\"))\n",
    "val_ds = Dataset.from_generator(make_gen_examples_train_val(val_images, val_anns),\n",
    "                                features=features)\n",
    "\n",
    "# Test\n",
    "test_images = sorted(glob.glob(\"./building-extraction-generalization-2024/test/image_512/*.tif\"))\n",
    "test_anns = sorted(glob.glob(\"./building-extraction-generalization-2024/test/image_512/*.tif\"))\n",
    "test_ds = Dataset.from_generator(make_gen_examples_train_val(test_images, test_anns),\n",
    "                                 features=features)\n",
    "\n",
    "\n",
    "ds_dict = DatasetDict({\"train\": train_ds, \"val\": val_ds, \"test\": test_ds})\n",
    "ds_dict.push_to_hub(\"tomascanivari/building_extraction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d6b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# Load Converted Dataset\n",
    "DATASET_HF_DIR = \"tomascanivari/building_extraction\"\n",
    "\n",
    "dataset = load_dataset(\"tomascanivari/building_extraction\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# Let's check first train image and annotation\n",
    "example = dataset[\"train\"][0]\n",
    "img = example[\"image\"]\n",
    "ann = example[\"annotation\"]\n",
    "\n",
    "# Load PIL image\n",
    "image = np.array(img.convert(\"RGB\"))\n",
    "annotation = np.array(ann)\n",
    "\n",
    "print(\"Number of Categories: \", np.unique(annotation[..., 0]))  # Red channel: category IDs\n",
    "print(\"Number of Instances: \", np.unique(annotation[..., 1]))  # Green channel: instance IDs\n",
    "\n",
    "# Plot the original image and the annotations\n",
    "plt.figure(figsize=(15, 5))\n",
    "for plot_index in range(3):\n",
    "    if plot_index == 0:\n",
    "        # If plot index is 0 display the original image\n",
    "        plot_image = image\n",
    "        title = \"Original\"\n",
    "    else:\n",
    "        # Else plot the annotation maps\n",
    "        plot_image = annotation[..., plot_index - 1]\n",
    "        title = [\"Class Map (R)\", \"Instance Map (G)\"][plot_index - 1]\n",
    "    # Plot the image\n",
    "    plt.subplot(1, 3, plot_index + 1)\n",
    "    plt.imshow(plot_image)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# Let' check instance 0\n",
    "print(\"Instance 1\")\n",
    "mask = (annotation[..., 1] == 1)\n",
    "visual_mask = (mask * 255).astype(np.uint8)\n",
    "Image.fromarray(visual_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52999be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (MaskFormerConfig, MaskFormerForInstanceSegmentation, MaskFormerImageProcessor)\n",
    "\n",
    "# Change label2id and id2label (Start from 0, when in annotations it starts from 1. Compatible with reduce in Processor)\n",
    "id2label = {0: 'building'}\n",
    "label2id = {'building': 0}\n",
    "\n",
    "# Load pre-trained weights\n",
    "model = MaskFormerForInstanceSegmentation.from_pretrained(\"facebook/maskformer-swin-base-coco\", id2label=id2label,\n",
    "                                                          ignore_mismatched_sizes=True)\n",
    "# Load processor\n",
    "processor = MaskFormerImageProcessor(\n",
    "    do_reduce_labels=True,\n",
    "    size=(512, 512),\n",
    "    ignore_index=255,\n",
    "    do_resize=False,\n",
    "    do_rescale=False,\n",
    "    do_normalize=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset # To make ImageSegmentationDataset a PyTorch dataset !!!!!!!!!!!!!!!!\n",
    "\n",
    "# Define the configurations of the transforms specific\n",
    "# to the dataset used\n",
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "# Build the augmentation transforms\n",
    "train_val_transform = A.Compose([\n",
    "    A.Resize(width=512, height=512),\n",
    "    A.HorizontalFlip(p=0.3),\n",
    "    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n",
    "    A.ToFloat()\n",
    "])\n",
    "\n",
    "class ImageSegmentationDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, transform=None):\n",
    "        # Initialize the dataset, processor, and transform variables\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return the number of datapoints\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert the PIL Image to a NumPy array\n",
    "        image = np.array(self.dataset[idx][\"image\"].convert(\"RGB\"))\n",
    "\n",
    "        # Get the pixel wise instance id and category id maps\n",
    "        # of shape (height, width)\n",
    "        annotation = np.array(self.dataset[idx][\"annotation\"])\n",
    "        instance_seg = np.array(annotation)[..., 1]\n",
    "        class_id_map = np.array(annotation)[..., 0]\n",
    "        class_labels = np.unique(class_id_map)\n",
    "        \n",
    "        # Build the instance to class dictionary\n",
    "        inst2class = {}\n",
    "        for label in class_labels:\n",
    "            instance_ids = np.unique(instance_seg[class_id_map == label])\n",
    "            inst2class.update({i: label for i in instance_ids})\n",
    "        # Apply transforms\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=instance_seg)\n",
    "            (image, instance_seg) = (transformed[\"image\"], transformed[\"mask\"])\n",
    "            \n",
    "            # Convert from channels last to channels first\n",
    "            image = image.transpose(2,0,1)\n",
    "        if class_labels.shape[0] == 1 and class_labels[0] == 0:\n",
    "            # If the image has no objects then it is skipped\n",
    "            inputs = self.processor([image], return_tensors=\"pt\")\n",
    "            inputs = {k:v.squeeze() for k,v in inputs.items()}\n",
    "            inputs[\"class_labels\"] = torch.tensor([0])\n",
    "            inputs[\"mask_labels\"] = torch.zeros(\n",
    "                (0, inputs[\"pixel_values\"].shape[-2], inputs[\"pixel_values\"].shape[-1])\n",
    "            )\n",
    "        else:\n",
    "            # Else use process the image with the segmentation maps\n",
    "            inputs = self.processor(\n",
    "                [image],\n",
    "                [instance_seg],\n",
    "                instance_id_to_semantic_id=inst2class,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            inputs = {\n",
    "                k:v.squeeze() if isinstance(v, torch.Tensor) else v[0] for k,v in inputs.items()\n",
    "            }\n",
    "        # Return the inputs\n",
    "        return inputs\n",
    "\n",
    "# Build the train and validation instance segmentation dataset\n",
    "train_dataset = ImageSegmentationDataset(\n",
    "    dataset[\"train\"],\n",
    "    processor=processor,\n",
    "    transform=train_val_transform\n",
    ")\n",
    "val_dataset = ImageSegmentationDataset(\n",
    "    dataset[\"val\"],\n",
    "    processor=processor,\n",
    "    transform=train_val_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if everything is preprocessed correctly\n",
    "print(\"Train Instance 0\")\n",
    "inputs = train_dataset[0]\n",
    "for k,v in inputs.items():\n",
    "  print(k, v.shape)\n",
    "\n",
    "print(\"\\nTrain Instance 1\")\n",
    "inputs = train_dataset[1]\n",
    "for k,v in inputs.items():\n",
    "  print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335da94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch])\n",
    "    pixel_mask = torch.stack([example[\"pixel_mask\"] for example in batch])\n",
    "    class_labels = [example[\"class_labels\"] for example in batch]\n",
    "    mask_labels = [example[\"mask_labels\"] for example in batch]\n",
    "    return {\"pixel_values\": pixel_values, \"pixel_mask\": pixel_mask, \"class_labels\": class_labels, \"mask_labels\": mask_labels}\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a285b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if batching is correct\n",
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k,v.shape)\n",
    "  else:\n",
    "    print(k,len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097c4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(\n",
    "          pixel_values=batch[\"pixel_values\"],\n",
    "          mask_labels=batch[\"mask_labels\"],\n",
    "          class_labels=batch[\"class_labels\"],\n",
    "      )\n",
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ec2d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set number of epochs and batch size\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch} | Training\")\n",
    "    \n",
    "    # Set model in training mode \n",
    "    model.train()\n",
    "    train_loss, val_loss = [], []\n",
    "    \n",
    "    \n",
    "    # Training loop\n",
    "    for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # Reset the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            pixel_values=batch[\"pixel_values\"].to(device),\n",
    "            mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "            class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "        )\n",
    "        # Backward propagation\n",
    "        loss = outputs.loss\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        # if idx % 50 == 0:\n",
    "            # print(\"  Training loss: \", round(sum(train_loss)/len(train_loss), 6))\n",
    "        \n",
    "        # Optimization\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Average train epoch loss\n",
    "    train_loss = sum(train_loss)/len(train_loss)\n",
    "    \n",
    "    # Set model in evaluation mode\n",
    "    model.eval()\n",
    "    start_idx = 0\n",
    "    print(f\"Epoch {epoch} | Validation\")\n",
    "    for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "                class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "            )\n",
    "            # Get validation loss\n",
    "            loss = outputs.loss\n",
    "            val_loss.append(loss.item())\n",
    "            # if idx % 50 == 0:\n",
    "                # print(\"  Validation loss: \", round(sum(val_loss)/len(val_loss), 6))\n",
    "    \n",
    "    # Average validation epoch loss\n",
    "    val_loss = sum(val_loss)/len(val_loss)\n",
    "    \n",
    "    # Print epoch losses\n",
    "    print(f\"Epoch {epoch} | train_loss: {train_loss} | validation_loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c178556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"models/mf\")\n",
    "# processor.save_pretrained(\"models/mf_p\")\n",
    "from transformers import MaskFormerForInstanceSegmentation, MaskFormerImageProcessor\n",
    "\n",
    "model = MaskFormerForInstanceSegmentation.from_pretrained(\"models/mf\")\n",
    "processor = MaskFormerImageProcessor.from_pretrained(\"models/mf_p\")\n",
    "\n",
    "# We won't be using albumentations to preprocess images for inference\n",
    "processor.do_normalize = True\n",
    "processor.do_resize = True\n",
    "processor.do_rescale = True\n",
    "\n",
    "# Push your model and preprocessor to the Hub\n",
    "model.push_to_hub(\"maskformer-swin-base-building-instance\")\n",
    "processor.push_to_hub(\"maskformer-swin-base-building-instance\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BuildingEnv",
   "language": "python",
   "name": "building_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d5d0089",
   "metadata": {},
   "source": [
    "Define Paths and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "997f4f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building-extraction-generalization-2024/train building-extraction-generalization-2024/train/train.json building_train\n",
      "building-extraction-generalization-2024/val building-extraction-generalization-2024/val/val.json building_val\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "def register_dataset_splits(dataset_name=\"building\", dataset_classes=[\"building\"]):\n",
    "    \n",
    "    def load_coco_json(json_file, image_root):\n",
    "        \"\"\"Load dataset in COCO format for Detectron2 visualization.\"\"\"\n",
    "        from detectron2.data.datasets import load_coco_json\n",
    "        dataset_dicts = load_coco_json(json_file, image_root)\n",
    "        return dataset_dicts\n",
    "\n",
    "    # --- PATHS ---\n",
    "    SPLITS = [\"train\", \"val\"]\n",
    "\n",
    "    for split in SPLITS:\n",
    "        DATA_DIR = Path(\"building-extraction-generalization-2024\") / split\n",
    "        ANN_PATH = DATA_DIR / f\"{split}.json\"\n",
    "\n",
    "        # Register dataset (if not already registered)\n",
    "        dataset_split_name = f\"{dataset_name}_{split}\"\n",
    "        if dataset_split_name not in DatasetCatalog.list():\n",
    "            DatasetCatalog.register(\n",
    "                dataset_split_name,\n",
    "                lambda ann_path=ANN_PATH, data_dir=DATA_DIR: load_coco_json(ann_path, data_dir)\n",
    "            )\n",
    "            MetadataCatalog.get(dataset_split_name).set(thing_classes=dataset_classes)\n",
    "            print(DATA_DIR, ANN_PATH, dataset_split_name)\n",
    "\n",
    "\n",
    "register_dataset_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac46b49a",
   "metadata": {},
   "source": [
    "Visualize Train Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_polygons(img, dataset_dict, metadata, outputs=None):\n",
    "    \"\"\"\n",
    "    Visualize polygons from a dataset dict.    \n",
    "    Args:\n",
    "        img: BGR image as np.array\n",
    "        dataset_dict: one dataset dict from DatasetCatalog\n",
    "        metadata: MetadataCatalog entry for the dataset\n",
    "    Returns:\n",
    "        img_vis: RGB image with drawn polygons\n",
    "    \"\"\"    \n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    v = Visualizer(\n",
    "        img,  # RGB\n",
    "        metadata=metadata,\n",
    "        scale=2.0,\n",
    "        instance_mode=ColorMode.IMAGE\n",
    "    )\n",
    "    \n",
    "    if outputs:\n",
    "        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    else:\n",
    "        out = v.draw_dataset_dict(dataset_dict)\n",
    "    \n",
    "    img_vis = out.get_image() \n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img_vis)\n",
    "    plt.title(f\"Image Name: {os.path.basename(img_path)} - ({len(dataset_dict['annotations'])} Annotations)\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    return img_vis\n",
    "\n",
    "\n",
    "# Load training dataset\n",
    "dataset_dicts = DatasetCatalog.get(\"building_train\")\n",
    "metadata = MetadataCatalog.get(\"building_train\")\n",
    "# Visualize first three image\n",
    "for d in dataset_dicts[0:3]:\n",
    "    img_path = d[\"file_name\"]\n",
    "    img = cv2.imread(img_path)\n",
    "    print(img_path)\n",
    "    img_vis = visualize_polygons(img, d, metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57510cb0",
   "metadata": {},
   "source": [
    "Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb6962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/19 18:29:39 d2.engine.defaults]: \u001b[0mModel:\n",
      "MaskFormer(\n",
      "  (backbone): D2SwinTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.013)\n",
      "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.026)\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.039)\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.052)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.065)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.078)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.091)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.104)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.117)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.130)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.143)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.157)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.170)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.183)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.196)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (12): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.209)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (13): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.222)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (14): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.235)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (15): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.248)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (16): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.261)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (17): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.274)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): PatchMerging(\n",
      "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): BasicLayer(\n",
      "        (blocks): ModuleList(\n",
      "          (0): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.287)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): WindowAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "              (softmax): Softmax(dim=-1)\n",
      "            )\n",
      "            (drop_path): DropPath(drop_prob=0.300)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm0): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (sem_seg_head): MaskFormerHead(\n",
      "    (pixel_decoder): MSDeformAttnPixelDecoder(\n",
      "      (input_proj): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (transformer): MSDeformAttnTransformerEncoderOnly(\n",
      "        (encoder): MSDeformAttnTransformerEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-5): 6 x MSDeformAttnTransformerEncoderLayer(\n",
      "              (self_attn): MSDeformAttn(\n",
      "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
      "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
      "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              )\n",
      "              (dropout1): Dropout(p=0.0, inplace=False)\n",
      "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (dropout2): Dropout(p=0.0, inplace=False)\n",
      "              (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (dropout3): Dropout(p=0.0, inplace=False)\n",
      "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (mask_features): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (adapter_1): Conv2d(\n",
      "        96, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (layer_1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (predictor): MultiScaleMaskedTransformerDecoder(\n",
      "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
      "          num_pos_feats: 128\n",
      "          temperature: 10000\n",
      "          normalize: True\n",
      "          scale: 6.283185307179586\n",
      "      (transformer_self_attention_layers): ModuleList(\n",
      "        (0-8): 9 x SelfAttentionLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (transformer_cross_attention_layers): ModuleList(\n",
      "        (0-8): 9 x CrossAttentionLayer(\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (transformer_ffn_layers): ModuleList(\n",
      "        (0-8): 9 x FFNLayer(\n",
      "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (query_feat): Embedding(100, 256)\n",
      "      (query_embed): Embedding(100, 256)\n",
      "      (level_embed): Embedding(3, 256)\n",
      "      (input_proj): ModuleList(\n",
      "        (0-2): 3 x Sequential()\n",
      "      )\n",
      "      (class_embed): Linear(in_features=256, out_features=81, bias=True)\n",
      "      (mask_embed): MLP(\n",
      "        (layers): ModuleList(\n",
      "          (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (criterion): Criterion SetCriterion\n",
      "      matcher: Matcher HungarianMatcher\n",
      "          cost_class: 2.0\n",
      "          cost_mask: 5.0\n",
      "          cost_dice: 5.0\n",
      "      losses: ['labels', 'masks']\n",
      "      weight_dict: {'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0, 'loss_ce_1': 2.0, 'loss_mask_1': 5.0, 'loss_dice_1': 5.0, 'loss_ce_2': 2.0, 'loss_mask_2': 5.0, 'loss_dice_2': 5.0, 'loss_ce_3': 2.0, 'loss_mask_3': 5.0, 'loss_dice_3': 5.0, 'loss_ce_4': 2.0, 'loss_mask_4': 5.0, 'loss_dice_4': 5.0, 'loss_ce_5': 2.0, 'loss_mask_5': 5.0, 'loss_dice_5': 5.0, 'loss_ce_6': 2.0, 'loss_mask_6': 5.0, 'loss_dice_6': 5.0, 'loss_ce_7': 2.0, 'loss_mask_7': 5.0, 'loss_dice_7': 5.0, 'loss_ce_8': 2.0, 'loss_mask_8': 5.0, 'loss_dice_8': 5.0}\n",
      "      num_classes: 80\n",
      "      eos_coef: 0.1\n",
      "      num_points: 12544\n",
      "      oversample_ratio: 3.0\n",
      "      importance_sample_ratio: 0.75\n",
      ")\n",
      "\u001b[32m[10/19 18:29:40 d2.data.datasets.coco]: \u001b[0mLoaded 3784 images in COCO format from building-extraction-generalization-2024/train/train.json\n",
      "\u001b[32m[10/19 18:29:41 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 3784 images left.\n",
      "\u001b[32m[10/19 18:29:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[10/19 18:29:41 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[10/19 18:29:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[10/19 18:29:41 d2.data.common]: \u001b[0mSerializing 3784 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/19 18:29:41 d2.data.common]: \u001b[0mSerialized dataset takes 11.02 MiB\n",
      "\u001b[32m[10/19 18:29:41 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=1\n",
      "\u001b[32m[10/19 18:29:41 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/instance/maskformer2_swin_small_bs16_50ep/model_final_1e7f22.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomas/Downloads/Siemens/building_env/lib/python3.10/site-packages/detectron2/engine/train_loop.py:474: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  grad_scaler = GradScaler()\n",
      "Weight format of MultiScaleMaskedTransformerDecoder have changed! Please upgrade your models. Applying automatic conversion now ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/19 18:29:41 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[4m\u001b[5m\u001b[31mERROR\u001b[0m \u001b[32m[10/19 18:29:42 d2.engine.train_loop]: \u001b[0mException during training:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tomas/Downloads/Siemens/building_env/lib/python3.10/site-packages/detectron2/engine/train_loop.py\", line 155, in train\n",
      "    self.run_step()\n",
      "  File \"/home/tomas/Downloads/Siemens/building_env/lib/python3.10/site-packages/detectron2/engine/defaults.py\", line 530, in run_step\n",
      "    self._trainer.run_step()\n",
      "  File \"/home/tomas/Downloads/Siemens/building_env/lib/python3.10/site-packages/detectron2/engine/train_loop.py\", line 494, in run_step\n",
      "    loss_dict = self.model(data)\n",
      "  File \"/home/tomas/Downloads/Siemens/building_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/tomas/Downloads/Siemens/building_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/tomas/Downloads/Siemens/Mask2Former/mask2former/maskformer_model.py\", line 198, in forward\n",
      "    outputs = self.sem_seg_head(features)\n",
      "  File \"/home/tomas/Downloads/Siemens/building_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/tomas/Downloads/Siemens/building_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/tomas/Downloads/Siemens/Mask2Former/mask2former/modeling/meta_arch/mask_former_head.py\", line 116, in forward\n",
      "    return self.layers(features, mask)\n",
      "  File \"/home/tomas/Downloads/Siemens/Mask2Former/mask2former/modeling/meta_arch/mask_former_head.py\", line 121, in layers\n",
      "    predictions = self.predictor(multi_scale_features, mask_features, mask)\n",
      "  File \"/home/tomas/Downloads/Siemens/building_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1775, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/tomas/Downloads/Siemens/building_env/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1786, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/tomas/Downloads/Siemens/Mask2Former/mask2former/modeling/transformer_decoder/mask2former_transformer_decoder.py\", line 418, in forward\n",
      "    outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[(i + 1) % self.num_feature_levels])\n",
      "  File \"/home/tomas/Downloads/Siemens/Mask2Former/mask2former/modeling/transformer_decoder/mask2former_transformer_decoder.py\", line 442, in forward_prediction_heads\n",
      "    attn_mask = F.interpolate(outputs_mask, size=attn_mask_target_size, mode=\"bilinear\", align_corners=False)\n",
      "  File \"/home/tomas/Downloads/Siemens/building_env/lib/python3.10/site-packages/torch/nn/functional.py\", line 4768, in interpolate\n",
      "    return torch._C._nn.upsample_bilinear2d(\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 5.79 GiB of which 12.75 MiB is free. Including non-PyTorch memory, this process has 5.04 GiB memory in use. Of the allocated memory 4.90 GiB is allocated by PyTorch, and 39.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[32m[10/19 18:29:42 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:00 (0:00:00 on hooks)\n",
      "\u001b[32m[10/19 18:29:42 d2.utils.events]: \u001b[0m iter: 0       lr: N/A  max_mem: 5013M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomas/Downloads/Siemens/building_env/lib/python3.10/site-packages/detectron2/engine/train_loop.py:493: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=self.precision):\n"
     ]
    }
   ],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.projects.deeplab import add_deeplab_config\n",
    "from mask2former import add_maskformer2_config\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "cfg = get_cfg()\n",
    "add_deeplab_config(cfg)\n",
    "add_maskformer2_config(cfg)\n",
    "cfg.merge_from_file(\"Mask2Former/configs/coco/instance-segmentation/swin/maskformer2_swin_small_bs16_50ep.yaml\")\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"building_train\",)\n",
    "cfg.DATASETS.TEST = (\"building_val\",)\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/instance/maskformer2_swin_small_bs16_50ep/model_final_1e7f22.pkl\"\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.OUTPUT_DIR = \"./train/mask2former_building\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "cfg.MODEL.MASK_ON = True\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # single class\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256 \n",
    "\n",
    "# --- Solver ---\n",
    "cfg.SOLVER.IMS_PER_BATCH = 1    # Batch Size\n",
    "cfg.SOLVER.BASE_LR = 0.00025    # Learning Rate\n",
    "cfg.SOLVER.MAX_ITER = 2000      # Number of Epochs\n",
    "cfg.SOLVER.STEPS = (1500,)      # Reduce LR at Epoch=1500\n",
    "cfg.SOLVER.GAMMA = 0.1\n",
    "\n",
    "cfg.SOLVER.CLIP_GRADIENTS.ENABLED = True\n",
    "cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE = \"norm\"  # or \"value\"\n",
    "cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE = 1.0   # typical default\n",
    "\n",
    "# --- Input Augmentation ---\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (640, 672, 704, 736) # Apply different resizes\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 1333                 \n",
    "cfg.INPUT.MIN_SIZE_TEST = 800                   # On validation always same resize\n",
    "\n",
    "# --- Output ---\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7 \n",
    "cfg.OUTPUT_DIR = \"./train/augmented_m2f\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea56557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.data import build_detection_train_loader\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "import os\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class AugmentedTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        # Define the augmentations\n",
    "        augmentation_list = [\n",
    "            T.RandomFlip(prob=0.5, horizontal=True, vertical=False),\n",
    "            T.RandomBrightness(0.8, 1.2),\n",
    "            T.RandomContrast(0.8, 1.2),\n",
    "            T.RandomSaturation(0.8, 1.2),\n",
    "        ]\n",
    "        \n",
    "        # Use a custom DatasetMapper with these augmentations\n",
    "        from detectron2.data import DatasetMapper\n",
    "        mapper = DatasetMapper(cfg, is_train=True, augmentations=augmentation_list)\n",
    "        \n",
    "        # Build the train loader\n",
    "        return build_detection_train_loader(cfg, mapper=mapper)\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"coco_eval\")\n",
    "        return COCOEvaluator(dataset_name, output_dir=output_folder)\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\n",
    "    \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"\n",
    "))\n",
    "\n",
    "# --- Dataset ---\n",
    "cfg.DATASETS.TRAIN = (\"building_train\",)\n",
    "cfg.DATASETS.TEST = (\"building_val\",)\n",
    "\n",
    "# --- DataLoader ---\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "\n",
    "# --- Model ---\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
    "    \"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"\n",
    ")\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # single class\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 \n",
    "\n",
    "# --- Solver ---\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2    # Batch Size\n",
    "cfg.SOLVER.BASE_LR = 0.00025    # Learning Rate\n",
    "cfg.SOLVER.MAX_ITER = 2000      # Number of Epochs\n",
    "cfg.SOLVER.STEPS = (1500,)      # Reduce LR at Epoch=1500\n",
    "cfg.SOLVER.GAMMA = 0.1\n",
    "\n",
    "# --- Input Augmentation ---\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = (640, 672, 704, 736) # Apply different resizes\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 1333                 \n",
    "cfg.INPUT.MIN_SIZE_TEST = 800                   # On validation always same resize\n",
    "\n",
    "# --- Output ---\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7 \n",
    "cfg.OUTPUT_DIR = \"./train/augmented_X101_512\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98fd0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27ddf03",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41264602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Load validation\n",
    "dataset_dicts = DatasetCatalog.get(\"building_val\")\n",
    "metadata = MetadataCatalog.get(\"building_val\")\n",
    "\n",
    "for d in random.sample(dataset_dicts, 3):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    visualize_polygons(im, d, metadata, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be612bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "def compute_max_f1(metrics, task=\"segm\"):\n",
    "    \"\"\"\n",
    "    Compute max F1 from COCO precision-recall curve.\n",
    "    task: \"segm\" or \"bbox\"\n",
    "    \"\"\"\n",
    "    precision = metrics[task][\"precision\"]  # shape: [IoU, recall, ...]\n",
    "    recall = metrics[task][\"recall\"]        # shape: [IoU, recall, ...]\n",
    "    \n",
    "    # AP50 corresponds to IoU=0.5, which is index 0 in COCOEvaluator (0.50:0.05:0.95)\n",
    "    iou_idx = 0\n",
    "    # Take precision and recall arrays for IoU=0.5\n",
    "    prec = precision[iou_idx].flatten()\n",
    "    rec = recall[iou_idx].flatten()\n",
    "    \n",
    "    f1_scores = 2 * (prec * rec) / (prec + rec + 1e-8)\n",
    "    return f1_scores.max()\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# Paths to your files\n",
    "gt_json = f\"{cfg.OUTPUT_DIR}/coco_eval/building_val_coco_format.json\"      # ground truth\n",
    "pred_json = f\"{cfg.OUTPUT_DIR}/coco_eval/coco_instances_results.json\"     # model predictions\n",
    "\n",
    "# Load COCO objects\n",
    "coco_gt = COCO(gt_json)\n",
    "coco_dt = coco_gt.loadRes(pred_json)  # this works because pred_json is in COCO format\n",
    "\n",
    "# Run COCO evaluation for segmentation ('segm') or bbox ('bbox')\n",
    "coco_eval = COCOeval(coco_gt, coco_dt, iouType='segm')  # or 'bbox'\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n",
    "\n",
    "# val_f1 = compute_max_f1(coco_eval)\n",
    "\n",
    "# print(\"Validation metrics:\")\n",
    "# print(\"AP50 (segm):\", val_metrics[\"segm\"][\"AP50\"])\n",
    "# print(\"Max F1 (segm):\", val_f1)\n",
    "\n",
    "# # Evaluate on training set\n",
    "# train_evaluator = COCOEvaluator(\"building_train\", output_dir=\"./output\")\n",
    "# train_loader = build_detection_test_loader(cfg, \"building_train\")\n",
    "# train_metrics = inference_on_dataset(trainer.model, train_loader, train_evaluator)\n",
    "# train_f1 = compute_max_f1(train_metrics)\n",
    "\n",
    "# print(\"\\nTraining metrics:\")\n",
    "# print(\"AP50 (segm):\", train_metrics[\"segm\"][\"AP50\"])\n",
    "# print(\"Max F1 (segm):\", train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6923c2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'size': [512, 512], 'counts': b'o_a31n?1O11O1OQP\\\\4'}, {'size': [512, 512], 'counts': b'i_d31n?3M2O2M2O0O1O100O1O100O100O1O11O1O1O2N1O2N1O2N1O1O2N1OQ`m3'}, {'size': [512, 512], 'counts': b'V_`41n?2N2N2N3M2O1N2N3M2N2N3N1N2N2N3M2N2O0O00000002N2N2O2O00001O0O1O2M2N2N3M2N2N3N1N2N3M2N2Nc`i2'}, {'size': [512, 512], 'counts': b'o_U71n?1O1O1O1001O1O1OO1O1O1O100O1O1O100O1O1O1O100O1O1O1002N1O1O1O1O1O1O1O1O1O1O1O1OR`6'}, {'size': [512, 512], 'counts': b'P_\\\\61n?2N2N3N1N2N3N1N3M2N2O2M2N3O000010O0001PASOk>l0SAUOn>n01O2N1O2N1O1O2N1O1O2N1O1O2N1O1O2N1O2M2N2NV`n0'}, {'size': [512, 512], 'counts': b'k^g62n?1N2N2N2N3M2O1N2N3M2N2N2O2M2N2N2N3M2O1N2N1O1O0001O2N3M2O1N2N2N2N2N2N2O1N2N2N2N2N2N2O1N2N2N2N2NSa`0'}, {'size': [512, 512], 'counts': b'Y]Z73l?2N2N2O1N3M2N2N2O1N3M2N2N2O1N3M2N2N2O1N3M2N2N2O1N3M2N2N2O1N3M2N2N2O1O2O0O1O1O101N1N2N2aC'}, {'size': [512, 512], 'counts': b'k_41n?2N3M2O0O1O100O1O1O100O1O1002N1O1O1O2N1O1O1O2N1O1OQ`^7'}, {'size': [512, 512], 'counts': b'oon01o?0O1FOd@2[?0c@1\\\\?2a@O^?3`@N_?:00O1O1O1O100O1O1O1002N1O1O1O1O1O2N1O1O1O1O1O1O2N1O1O1O1O1OQ`^6'}, {'size': [512, 512], 'counts': b'`o<3l?2N2N2O1N2N2N2N2O1N2N2N2N2O0O1O1O100O1O1O1O10O01O000001O000001O01O0000002N2O1N3M2N2N3M2N2O2M2N2N3M2N2NkPi6'}, {'size': [512, 512], 'counts': b']mh13l?2N2O1N2N3M2O1N2N201O0001O00010O00001O010O00001O01O01O00001N1O1N3M2N3M2N2NfQd5'}, {'size': [512, 512], 'counts': b'Y]W11n?2N2N2O2M2N2N2N3N1N2N2N3N1N2N2N01O0001O01O0001O01O0001O01O3N1N2N2N2O2M2N2N2O1N3M2N2O1N2NlRR6'}, {'size': [512, 512], 'counts': b'j[h13l?2N2N2O1N2N2N2N3N1N2N2N2^AWOk=j0SBXOk=j0SBXOk=j0SBXOk=k0RBWOl=k0RBXOl=i0SBXOk=j0SBXOk=j0SBXOk=j0SBYOj=i0TBYOk=_1N2N3M2O11O01O0000010O00000010O0000000N3M2N2N2N2O1N00001O0001O01O000010O0001O000010O0001O00010O0000001O01O01O000010O0001O3M2N2O1N2N2N2N3N1N2N2N2N2N2O1N3M2N2N2O1N2N2N3M2N2O1N2N2N2NjS_4'}, {'size': [512, 512], 'counts': b'Q\\\\h02n?1N2N2N3M2N2N3N1N2N3M2N2N2N3N1N2N3M2N2N3M2O1N2N3M2N2N3O000001M2N2N2O2M2N2N3M2O1N3M2N2O2M2N2N3N1N2N3M2O1N3M2N2O2M2N2N_SZ6'}, {'size': [512, 512], 'counts': b'V[;1n?2N2N2O1N3M2N2O1N2N3M2O1N2N2N01O0001O01O01O0001O01O00010O0000010O000011N2N3M2O2M2N2O2M2N3M2O2M2NTUl6'}, {'size': [512, 512], 'counts': b'\\\\jk11n?2N2O1N3M2O1N2N3N1N2N2O2M2N2O101O000000001O000000001O000000001N10000N2OO001O002N2O1N2N3M2N2O2M2N2N3M2O1N3M2N2NlTX5'}, {'size': [512, 512], 'counts': b'oo11n?1O1O1O11O1O2NQPj7'}, {'size': [512, 512], 'counts': b'_?3n?Na`n7'}, {'size': [512, 512], 'counts': b'c>l0V?O0O1O1N2N3M2N2N2N2N3M2N2N2NmPh7'}, {'size': [512, 512], 'counts': b'c<n0S?01O00N1O001O02N2N2N3M2N3M2N2N3M2N2N3M2NmRe7'}, {'size': [512, 512], 'counts': b']81o0d0[=^OcBc0\\\\=_ObBa0^=B_B>a=D^B;b=H[B8f=IXB7h=KVB5j=NTB2k=0SB0m=3PBMP>5nAKS>6lAIT>:iAFW><gADY>Q101O01O000000000010O00000002N2N3M2O1N2N2N3M2N2N2O1N3M2N2N2N3M2N2O1N2N3M2N2N2N^gT7'}, {'size': [512, 512], 'counts': b'k^d21n?2N2N3N1N2N3M2O2M2N2N3N1N2N3M2O2M2N2N3N1N2N3M2O1N000002O1N2N3M2O1N2N3N1N2N3M2O1N3M2O1N3M2N2O2M2N2N3N1N2NkPa4'}, {'size': [512, 512], 'counts': b'T^f41n?2N2O2M2N2N3N1N2N3N1N3M2O1N3M2O1N3M2N2O2M2N2O2M2N2O2M2N201O0001N1O1N3M2N2gNbAm0a>POaAn0a>PObAm0`>QObAm0k>M2O1N001O001O002N2N2O1N3M2N2N3M2N2N[Q\\\\2'}, {'size': [512, 512], 'counts': b'm][32n?1N2N2N3M2N2O1N3M2N2N2N3M2O1N2N3M2N2N201O000001O0N2N1O01O0000001O000O1102M2N2N2O1N3M2N2O1N2N3M2O1N2N2N3N1N2N2Nnah3'}, {'size': [512, 512], 'counts': b'Q]k31n?2N2O2M2O1N3N1N2N3N1N2O2M2N2O2M2O1N3M2O1N3lAgN^=Y1`BiN`=W1_BkN`=V1]BlNd=S1ZBoNf=R1XBmNj=S1TBmNn=S1QBlNR>T1kAmNV>S1iAlNY>T1eAlN^>Y12O1N3M2N2O2M2N2N3N1Hn@_OT??n@_OU?>m@@U?>8O2M2N2N3N1N2NURZ3'}, {'size': [512, 512], 'counts': b'olW51n?2N2O1N2N3N1N2N2O2M2N2O1N2O2M2N2O1N3M2000000010O000010O0001O0001O01O00001M2N2N3M2N2O2M2N2N2N3M2O1N3M2N2NUbm1'}, {'size': [512, 512], 'counts': b'ZlR41n?2N2N3M2O2M2N3M2O1N3M2N2O1N3M2N2O20O0001O00010O00001O01O01O000010O0001O000001M2N2O1N2N3N1N2N2O1N3M2O1N2N2O2M2N2O1N2Ncbo2'}, {'size': [512, 512], 'counts': b'^lo41n?2N2N3M2O1N2N0000010O00000010O0001O2O1N2N2N2N2O1N00000001O01O000001O01O0001O3M2N2O1N2N2N3M2O1N2N2N2NSdV2'}, {'size': [512, 512], 'counts': b'o[j41n?2N2N3N1N2N3M2O1N3M2N2OO00001O01O0001O0001O01O0000010O0000010O00001O01O0001O3M2O1N2N2N3M2O1N2N2NbT]2'}, {'size': [512, 512], 'counts': b'a[e42n?1N2N2N3M2N2N3M2N2N2N01O0001O0000001O0000001O01O01O0000001O00000010O3M2N2O1N2N2N2O2M2N2O1N2N2Nodb2'}, {'size': [512, 512], 'counts': b'^ZX52n?1N2N2N3M2N2N3M2O1N3M2N2N3M2N201O000010O000010O00010O00010O000010O00001M2N2N3M2O1N2N3M2O1N3M2N2O2M2N2Nbdm1'}, {'size': [512, 512], 'counts': b'bla51n?2N2N2N3N1N2N3M2O1N3M2N2N2O2M2N2N3O000010O0001O01O00010O00010O00000N3M2N2N3N1N2N3M2O1N2N3M2N2O2M2N2Ncbd1'}, {'size': [512, 512], 'counts': b'Vlm51n?2N2DMh@6U?Lj@5T?Mj@5T?Mj@6T?Kj@7T?<N3M2N2O2M2N2N3M20001O01O01O0N3M2N2N2N01O0001O0000010O0000000010O000001O01O3M2O1N2N2N2O1N3M2N2O1N2N2N2O1N3M2N2O1N2N2N2O2M2N2N2O1N2N2Nncl0'}, {'size': [512, 512], 'counts': b'PZ`52n?1N2N2N3M2N2N3M2O1N3M2N2N3M2N2N201O00010O00001O01O0001O01O01O000010O00000N3M2N2N2O2M2N2N2N2O2M2N2N2N3N1N2N2NQUd1'}, {'size': [512, 512], 'counts': b'eYY72n?1N2N2N2N3M2O1N2N2N3M2N2O1N2N3M2N2N2O110O0000000010O00000001O0001O0001O00000001M2@WAFk>8WAFk>9VAFk>8WAFk>8WAFf4'}, {'size': [512, 512], 'counts': b'dik53l?2N2N2N2O1N2N2N2N2N2O2M2N2N2N2N2000000001O00010O0000001O00001O00000N3M2N2N3N1N2N3M2N2O1N3M2N2NdU\\\\1'}, {'size': [512, 512], 'counts': b'kYb61n?2N3M2O2M2N3M2N3N2M2N3M2O1N000001O0001O01O00000010O000001O0001O01O2N2N2N2O0O00000000010O0000L^Ok@b0U?401O01O000000011N2N2N3M2N2O2M2N2N3M2NPW<'}, {'size': [512, 512], 'counts': b']X]63l?2N2N2N2O1N2N3M2N2O1N2N2N2N3M2O1N2N2N2N2OO00000001O01O0[O\\\\AOd>2]AMb>3aAJ_>6cAH^>7eAF^>7dAH]>6fAG\\\\>7gAF[>9fAE\\\\>9h0M2N2N2NShm0'}, {'size': [512, 512], 'counts': b'cVi53l?2N2O1N2N2N2O1N2N3M2O1o@ZOe>h0YAZOe>h0ZAYOd>i0ZAZOd>g0[AZOc>h0[A[Ob>S1N3N1N2N3M2O2M2N1O010O000002N2N2N3M2N2N3@`A[Ob>c0aAZOa>d0aAZOb>c0`A[Ob>c0`A[Ob>c0`A[Oc>b0?N2N3M2N2N3M2N2NZi\\\\1'}, {'size': [512, 512], 'counts': b'me[73l?2N2N2O1N2e@En>=PAEn>=PAEn>=QADn>=PAEn>=PAFm>h0N2O1N3M2N2N2O1N3M2N2N2O2M2N2N2O1N3M2N2N2O2M2N2N2N2O2M2N2N2RK'}, {'size': [512, 512], 'counts': b'[Ta62n?1N2N2O1N2O1N2N2N2N2O1N2N2N2N2O1N2N2N2N2O1N2N1O2N2O1N000000010O000000010O0000000102M2N2N2N2O1N2N2N3M2O1N2N2N2N2O1N3M2N2N2O1N2N2Nk[>'}, {'size': [512, 512], 'counts': b'cim32n?1N3M2N3M2N3M2O1N001O00001O010O002N3M3M3M2N3N2M3M3M3M3M2N01O0001O0000001O01O01O0000001O01O0001O00001O2O02O000O100O100O2O000O1N2N2N2O1N3M2N2O1N2N2N2O1N3M2N2O1N2N2N2O2M2N2N2O1N2N2N`Ve2'}, {'size': [512, 512], 'counts': b'TY`22n?1N2N3M2N3M2N2O2M2N2N3M2N3M2N2O2M2N2N00000000010O000000000010O2N3M2N2N2O1N2N2N2N2N2O1N2N2N2N2N2O1N2N2N2NQWe4'}, {'size': [512, 512], 'counts': b'Vhg41o?2M2N2N2N2N2O1N3M2N2N2N2N2O1N2N3M2N2N2O100000010O000001O000010O0001O0000001O0N2N3M2N2O2M2N2N3N1N2N3M2N2O2M2N2NnV\\\\2'}, {'size': [512, 512], 'counts': b'ZX`31n?2N2O1N3N1N2N2O2M2N2J]Om@e0P?^Om@d0Q?7N2N3N1N2N2N3M2O1N2N000002O1N2N3M2O2M2N2N3N1N3M2N2O2M2N2N3N1N3M2O1N3M2N`Wg3'}, {'size': [512, 512], 'counts': b'[Wk33l?2O1N2N3N1N2N3N1N3M2O1N3M201O00010O01O01O001O01O01O0001O000O1N3M2O1N2N3N1N2N2O2M2N2O2M2NhW^3'}, {'size': [512, 512], 'counts': b'jWS51n?2N2N2O1N3M2N2O1N3M2N2O1N3M2N2O1N3M2N2O1N2N00010O001O01O01O00010O001O01O01O0002O1N3N1N3M2O1N3M2O2M2O2M2N3N1N3M2O2M2O2M2NWXn1'}, {'size': [512, 512], 'counts': b'XgT42n?1N2N2N2N3M2O1N2N3M2N2N2O2M2N2N2N3M2O1N2N1O001O01O00000000001O00000000001O3M2N2O1N3M2N2N2N3M2N2N3M2N2N2N3M2N2NPYo2'}, {'size': [512, 512], 'counts': b'Qfa21n?2N2N2O1N3M2N2O1N3M2O1N2N3M2O1N2N3M2O1N2N3M2O1N2N3N1000001O0O11O01O0001O000N2N3M2O1N3M2N2O1N3M2N2O1N3M2N2O2M3M3M3N2M3M3MUi_4'}, {'size': [512, 512], 'counts': b'Yfl42m?2O2M2N2N3M2N3M2N3M2N3M2N2N3M2N3M2N2N000000000011N2N2N2N3M2O1N2N2N2N3M2O1N2N2N2N3M2O1N2N2N2Nfi[2'}, {'size': [512, 512], 'counts': b'ceQ31n?2N2N2N3N1N2N2N3M2O1N3M2N2O110O00000010O000O1O1N2N3M2N2N2N2N3M2N2N2N2NPZ\\\\4'}, {'size': [512, 512], 'counts': b'dZg11n?2N2O1N3N1N2N3O00001O01O01O0001O0N2N2N3N1N2N2Nodl5'}, {'size': [512, 512], 'counts': b'gYU23l?2N2N2N2O1N2N2N2N2N2O2M2N2N2N2N2O1N2N2N1O11N2N2N3M2N3N1N2N3M2N2N3M2O2M2N2N3M2NTfV5'}, {'size': [512, 512], 'counts': b'\\\\i`02n?1N2N2N3M2N2O1N3M2N2N2N3N1N2N2N3M2N2O1N3M2N2N2N20000000N2N3M2N2O1N2N3M2O1N2N3M2O1N2N2N3M2O1N2N2N3N1N2N2NXfd6'}, {'size': [512, 512], 'counts': b'[hi11n?2N2N2O2M2N2O2M2N2O1N3M2O1N3M2N2O0O0001O00002N2N3M3M2N3M3M2N3M3M2N3Mcgd5'}, {'size': [512, 512], 'counts': b'dWb01n?2N2N2N3M2O1N2N3M2N2N2O2M2N2N2N3N11O0001O00000000N2N3M2N2O1N2N2N2N2O2M2N2N2N2O1N2N2NoWh6'}, {'size': [512, 512], 'counts': b'Qgl12m?2N2N2N3M2N2N2M4N1N2N2N3M2O1N3M2N2O1N3M2N2N2O2M2N2N201O0001O0N2N2N3N1N2N3M2O1N3M2N2O2M2N2N3N1N2N3M2O1N3M2N2O2M2N2N_XV5'}, {'size': [512, 512], 'counts': b'Xe^11n?2N2N2O2M2N2O2M2N2O2M2N2N2O2M2N2O2M2N20001O01O01O01O010O000010O00010O01O000N3M2N2N3N1N2N3M2O2M2N2N3M2O1N3M2Nhie5'}, {'size': [512, 512], 'counts': b'_dm11n?2N2N2O2M2N2O2M2N2N3N1N2N3N1N2N3M2O1N2N3N1N2N3M2O1N3M20001O0001O0N2N3N1N2N3N1N2O1N3M2O101N1O1N3M2N2N3M2N2N2O2M2N2N3M2N2NljS5'}, {'size': [512, 512], 'counts': b'ZS=1o?2M2N2N2N2N3M2O1N2N2N3M2N2000000010O000000N3M2N2N2O1N2N0001O01O0001O3N1N2N2N2O1N2N3M2O1N2N2N2O1N3M2N2O1N2N2N\\\\\\\\g6'}, {'size': [512, 512], 'counts': b'o`]11n?2N2N3M2O2M2N2N3N1N3M2N2N3N1N3M2N2N3N1N3M2N2O2M2N2N3M2O2M2N2N3N110O00001O010O00001O0001M2N2N2O1N2N3M2O1N2N2N3N1N2N2N2O1N3M2N2O1N2N2N3L3K5K5K5K`^^5'}, {'size': [512, 512], 'counts': b'hel32n?1N2N2N3M2N2N1O1O000002N2N2N2N3M2N2N[ji3'}, {'size': [512, 512], 'counts': b'eU[53l?2O1N2N2N2O2M2N2O1N2N2O2M2N2O1N2N2O1N3M2O1N2N2N01O000010O0001O01OH^ASOd>k0_AROc>l0`AQOb>m0;M2O2M2N3M2O2M2N2N3N1N3M2N]jk1'}, {'size': [512, 512], 'counts': b'PeV32n?2M2N3M2N3M2N2O1N2N3M2N2N2N20001O0001O0001O01O00001O01O1O010O1O10O01OO1O2M2N2N2N3M2N2N3M2N2N2N3M2N2NPjo3'}, {'size': [512, 512], 'counts': b'dT^31n?2N2N2N3N1N2N2N3M2O1N3M2N20001O01O01O01O0010O0001O01O01O0O2N1N2N3M2N2N3M2N2N3M2NdZm3'}, {'size': [512, 512], 'counts': b'cd_41n?2N2N3M2O1N3M2N2O2M2N2N01O0000010O000000010O00000010O000002N2O1N2N3M2N2O1N3M2N2Nhkk2'}, {'size': [512, 512], 'counts': b'kc[21n?2N2N2O2M2N2O2M2N2N3N1N2N2O2M2N2N3N1N2N3N1N2N3M2O1N2N3N1O1O2O0O1O0000O1O101N1O100O1O100O2N100O100O1O2O0O1O100O1O101N1O100O1O101N100O1O100O1O2O0O1O100O1O101N1O100O100O2N1O1O1O1O1O1O2N1O1O2N3M4LZlS4'}, {'size': [512, 512], 'counts': b'aSn39g?1O1O101N1O1O1O2N1O101N1O1O1O2N1O1O2O0O1O1O2N1O1O00O2M2O1N2O1O1N2O2N1K5I7Ial^3'}, {'size': [512, 512], 'counts': b'jS^61n?3L3M3N2N2N2N2O1N2N2O1N2N01O001O01O01O000010O0001O00010O00001O010O00001O3N1N2N2N2N3N1N2N2N2Nf\\\\j0'}, {'size': [512, 512], 'counts': b'ZS[71n?2N2N2N3N1N2N2N3N1N2N3M2O1N2N3M2O1N2N3M2O1N2N000001O01O0001O0001O01O0000010O000000gM'}, {'size': [512, 512], 'counts': b'TcZ62m?2N2O1N2N2O1N2N2N2O1N2N3M01O01O01O01O00010O0010O0001O01O01O00010O0002O1N3M2O1N3N1N3M2N3M2NW]n0'}, {'size': [512, 512], 'counts': b'YRm61n?2N2N3N1N2N2N3N1N2N3M2O1N3M2O110O00001O0001O0N2N2N2O1N3M2O1N2N2O1N3M2N2O1N2NX]?'}, {'size': [512, 512], 'counts': b'mQU73l?2N2O1N2N2N3N1N2N2O1N2N3M2O1N2N2N2O2M2N20000000N2N3M2N2O1N2N3M2O1N2N3M2O1N2N2N3N1N2N2Nhm4'}, {'size': [512, 512], 'counts': b'^a_71n?2N2N2O1N3M2N2O1N2N3M2O1N2N3N1N2N2N2O20O0000000N2N2N3N1N2N2O1N2NbN'}, {'size': [512, 512], 'counts': b'fQU61n?2N3\\\\OKXA7R>IQB3J6S>IQB4I5T>IRB4G5V>IPB5G4W>IPB6F3X>JPBg0o=ZOoAh0o=ZOPBg0n=\\\\OoAf0Q>ZOmAh0S>c000001O0N2N2N3M2O1N2N3M2N2O2M2N2N2N3N1N2N2N3M2O1N2N3M2N2O1N2N2N2Na^S1'}, {'size': [512, 512], 'counts': b'o`i73l?2O1N2N2O1N2N3N1N2O1N2N2XO'}, {'size': [512, 512], 'counts': b'UP]62n?1N2N2O2M2O1N2N3N1O1O2N1O1O1O2N1O1O2N1000001O01O000N3M2N2N3N1N2N2N3M2O1N3M2N2NXon0'}, {'size': [512, 512], 'counts': b'PPg61o?1O1O1O2N1O1O2N1O1O2N1O1O1O2N1O1O2N1O1O1O2N1O1O1O11M2N2N2N2N2O2M2N2N2N2O2M2N2N2N__d0'}, {'size': [512, 512], 'counts': b'UQe01n?2N2N3N1N3M2N3N1N3M2O1N3M2N3N1N1O0001O0000001O00001O2N3M2N2N2N2N2N2N2N2N2N2N2M3NP_f6'}, {'size': [512, 512], 'counts': b'SP11n?2N2O1N3N1O1O2N1O1O2N1O1O1O2N1O1O2N11O0001O00001O0001O0N2N2N2O1N2N2N3N1N2N2N2O1N2NWoY7'}, {'size': [512, 512], 'counts': b'aPR11n?2O2M2N2N2N2N2N2N2N2N2N2N2N2N2N000001O100O1O1O100O1O1O100O1O1O1O100O1O1O100O1O1O1O100O1O3M2O1N2N2NkoT6'}, {'size': [512, 512], 'counts': b'PPm11o?1O1O1O2N1O1O2N1O1O1O2N1O1O2N1O1O2N1O1O1O2N1O1O0000O1O1O2N2N2N3M2O1N3M2N2N3M2N2Nb_^5'}, {'size': [512, 512], 'counts': b'P`:2n?1O1O1O1O1O1O1O2N1O1O1O1O1O1O1O2N1O1O1O1O100000N2N2N3M2O1N2N2N3N1N2N2Na_S7'}, {'size': [512, 512], 'counts': b'P`e01o?1O1O1O1O2N1O1O1O1O2N1O1O1O1OO1O1O1O1O101N2N2N3M2Nk_m6'}, {'size': [512, 512], 'counts': b'PPP11o?1O1O2N1O1OO1O1O100O1O100OQPi6'}, {'size': [512, 512], 'counts': b'PPT72n?1O1O1O1O1O1OO1O1O100O1O1O100OQPd0'}, {'size': [512, 512], 'counts': b'PP\\\\61o?1O1OO1OQ`a1'}, {'size': [512, 512], 'counts': b'Q4T1k>3M2N2N20001O0001O0001O01O000010O000001N1O1N3M2N2N2N3M2O1N2N3M2N2N3M2O1N2N3M2N2NgZ[7'}, {'size': [512, 512], 'counts': b'S3j0W?O1N2N3M2N2N3N1N2N3M2N2N_\\\\i7'}, {'size': [512, 512], 'counts': b'^2?a?1N1O0000000001O01O0000000001O01O0000000002N2O1N2N3M2N2N2Nlm`7'}, {'size': [512, 512], 'counts': b'f1h0X?0O0000000000001O01O3M2N2N2N2O1N3M2N2N2N2NVnd7'}, {'size': [512, 512], 'counts': b'R1d0]?O1N2N3M2N2N3M2N2Ndnj7'}]\n",
      "F1-score evaluation (IoU >= 0.5):\n",
      "{'TP': 29945, 'FP': 46161, 'FN': 7768, 'precision': 0.3934643786298058, 'recall': 0.7940232810967041, 'f1': 0.5261863133571725}\n"
     ]
    }
   ],
   "source": [
    "# compute_f1_from_coco.py\n",
    "import json\n",
    "import numpy as np\n",
    "from pycocotools import mask as mask_utils\n",
    "from shapely.geometry import Polygon\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "gt_json = f\"./train/augmented_X101_512/coco_eval/building_val_coco_format.json\"      # ground truth\n",
    "pred_json = f\"./train/augmented_X101_512/coco_eval/coco_instances_results.json\"     # model predictions\n",
    "\n",
    "with open(gt_json, \"r\") as f:\n",
    "    gt_coco = json.load(f)\n",
    "\n",
    "with open(pred_json, \"r\") as f:\n",
    "    pred_coco = json.load(f)\n",
    "\n",
    "# --- Build image_id -> (height, width) mapping ---\n",
    "img_sizes = {img[\"id\"]: (img[\"height\"], img[\"width\"]) for img in gt_coco[\"images\"]}\n",
    "\n",
    "# --- Convert COCO JSON to {image_id: [RLEs]} format ---\n",
    "def coco_to_rle(coco_json, img_sizes):\n",
    "    rle_dict = {}\n",
    "    for ann in coco_json[\"annotations\"]:\n",
    "        image_id = ann[\"image_id\"]\n",
    "        seg = ann[\"segmentation\"]\n",
    "        height, width = img_sizes[image_id]\n",
    "\n",
    "        # Convert polygon to RLE if needed\n",
    "        if isinstance(seg, dict) and \"counts\" in seg:\n",
    "            rle = seg  # already RLE\n",
    "        else:\n",
    "            # polygon -> RLE\n",
    "            rle = mask_utils.frPyObjects(seg, height, width)\n",
    "            if isinstance(rle, list):\n",
    "                rle = mask_utils.merge(rle)\n",
    "        rle_dict.setdefault(image_id, []).append(rle)\n",
    "    return rle_dict\n",
    "\n",
    "gt_rles = coco_to_rle(gt_coco, img_sizes)\n",
    "\n",
    "print(gt_rles[1])\n",
    "\n",
    "pred_rles = {}\n",
    "for ann in pred_coco:\n",
    "    image_id = ann[\"image_id\"]\n",
    "    seg = ann[\"segmentation\"]\n",
    "    if isinstance(seg, dict) and \"counts\" in seg:\n",
    "        rle = seg\n",
    "    else:\n",
    "        rle = mask_utils.frPyObjects(seg, ann[\"height\"], ann[\"width\"])\n",
    "        rle = mask_utils.merge(rle) if isinstance(rle, list) else rle\n",
    "    pred_rles.setdefault(image_id, []).append(rle)\n",
    "\n",
    "# --- Compute F1 ---\n",
    "def compute_f1(gt_rles, pred_rles, iou_thresh=0.5):\n",
    "    total_TP = total_FP = total_FN = 0\n",
    "    for img_id in set(list(gt_rles.keys()) + list(pred_rles.keys())):\n",
    "        gt_list = gt_rles.get(img_id, [])\n",
    "        pr_list = pred_rles.get(img_id, [])\n",
    "\n",
    "        if len(gt_list) == 0 and len(pr_list) == 0:\n",
    "            continue\n",
    "\n",
    "        if len(gt_list) == 0:\n",
    "            total_FP += len(pr_list)\n",
    "            continue\n",
    "        if len(pr_list) == 0:\n",
    "            total_FN += len(gt_list)\n",
    "            continue\n",
    "\n",
    "        # Build IoU matrix\n",
    "        ious = mask_utils.iou(pr_list, gt_list, [0]*len(gt_list))  # [pred, gt]\n",
    "\n",
    "        # Hungarian assignment to maximize IoU\n",
    "        cost = -ious\n",
    "        row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "        matched_pred = set()\n",
    "        matched_gt = set()\n",
    "        for r, c in zip(row_ind, col_ind):\n",
    "            if ious[r, c] >= iou_thresh:\n",
    "                matched_pred.add(r)\n",
    "                matched_gt.add(c)\n",
    "\n",
    "        total_TP += len(matched_gt)\n",
    "        total_FP += len(pr_list) - len(matched_pred)\n",
    "        total_FN += len(gt_list) - len(matched_gt)\n",
    "\n",
    "    precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0.0\n",
    "    recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"TP\": total_TP,\n",
    "        \"FP\": total_FP,\n",
    "        \"FN\": total_FN,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "metrics = compute_f1(gt_rles, pred_rles, iou_thresh=0.5)\n",
    "print(\"F1-score evaluation (IoU >= 0.5):\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa3d4a",
   "metadata": {},
   "source": [
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab6a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# --- Paths ---\n",
    "test_folder = Path(\"building-extraction-generalization-2024/test/image\")\n",
    "output_csv = \"submission.csv\"\n",
    "\n",
    "# --- Helper: binary mask to polygon coordinates ---\n",
    "def mask_to_coords(mask):\n",
    "    \"\"\"Convert a HxW binary mask (numpy array) to polygon coordinates.\"\"\"\n",
    "    mask = mask.astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    coords_list = []\n",
    "    for cnt in contours:\n",
    "        if len(cnt) >= 3:  # valid polygon\n",
    "            coords_list.append([(int(x), int(y)) for [[x, y]] in cnt])\n",
    "    return coords_list\n",
    "\n",
    "# --- Run inference & prepare CSV ---\n",
    "rows = []\n",
    "for img_path in sorted(test_folder.iterdir()):\n",
    "    print(img_path)\n",
    "    img_id = int(img_path.stem)  # ImageID\n",
    "    img = cv2.imread(str(img_path))\n",
    "    \n",
    "    outputs = predictor(img)[\"instances\"]\n",
    "\n",
    "    masks = outputs.pred_masks.cpu().numpy() if len(outputs) > 0 else []\n",
    "    \n",
    "    all_coords = []\n",
    "    for mask in masks:\n",
    "        polys = mask_to_coords(mask)\n",
    "        all_coords.extend(polys)\n",
    "    \n",
    "    # If no detection\n",
    "    if not all_coords:\n",
    "        all_coords = []\n",
    "\n",
    "    rows.append({\"ImageID\": img_id, \"Coordinates\": str(all_coords)})\n",
    "\n",
    "# --- Save CSV ---\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"Submission CSV saved as {output_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BuildingEnv",
   "language": "python",
   "name": "building_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
